{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,


   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as comp\n",
    "from typing import NamedTuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data() -> NamedTuple('ReadOutput', [('ReadOutput', dict)]): \n",
    "    DATABASE_CLOUDSQL=\n",
    "    USER_CLOUDSQL=\n",
    "    PASSWORD_CLOUDSQL=\n",
    "    URL=\n",
    "    PORT=\n",
    "    bucket_name=\n",
    "    folder= \n",
    "    source_file_name= \n",
    "    destination_blob_name= \n",
    "    test_size=0.25\n",
    "    quant=[0.2, 0.4, 0.6, 0.8, 1]\n",
    "    column_bin='sepal length (cm)'\n",
    "    target='target'\n",
    "    \n",
    "    import itertools\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import logging\n",
    "    import scikitplot as skplt\n",
    "\n",
    "    from sklearn import datasets\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "    from matplotlib.backends.backend_pdf import PdfPages\n",
    "    from google.cloud import storage\n",
    "\n",
    "    def extract_cloudsql(DATABASE_CLOUDSQL:str,USER_CLOUDSQL:str,PASSWORD_CLOUDSQL:str,URL:str,PORT:str)-> pd.DataFrame: \n",
    "    con_cloudsql = psycopg2.connect(\n",
    "            database=DATABASE_CLOUDSQL\n",
    "            ,user=USER_CLOUDSQL\n",
    "            ,password=PASSWORD_CLOUDSQL\n",
    "            ,host=URL\n",
    "            ,port=PORT)\n",
    "    sql = (\n",
    "        '''\n",
    "            SELECT \n",
    "                *\n",
    "            FROM iris_dataset \n",
    "        '''\n",
    "        )\n",
    "    df = pd.read_sql_query(sql, con_cloudsql)\n",
    "    return df\n",
    "\n",
    "    def read_data(): \n",
    "        iris = datasets.load_iris()\n",
    "        df_iris = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                         columns= iris['feature_names'] + ['target'])\n",
    "        return df_iris\n",
    "\n",
    "    def feature_enginering(df:pd.DataFrame,quant:list,column:str)-> pd.DataFrame: \n",
    "        binning_values=[]\n",
    "        for c in quant:\n",
    "            binning_values.append(np.round(np.nanquantile(df_iris[column], c),2))\n",
    "        for bin_value in binning_values:\n",
    "            df_iris[column+'>'+str(bin_value)]=(df_iris[column]>float(bin_value)).astype(int)\n",
    "        return df_iris\n",
    "\n",
    "    def split_dataframe(df:pd.DataFrame,test_size:float,target:str): \n",
    "        X_train, X_test, y_train, y_test = train_test_split(df.drop([target],axis=1), \n",
    "                                                        df[target], \n",
    "                                                        test_size=test_size, \n",
    "                                                        random_state=0)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def push_gcp(df:pd.DataFrame,bucket_name:str,source_file_name:str,destination_blob_name:str)-> str:\n",
    "        df.to_csv(source_file_name)\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(bucket_name)\n",
    "        blob = bucket.blob(destination_blob_name)\n",
    "        blob.upload_from_filename(source_file_name)\n",
    "        logging.info('{} is pushed to gcp'.format(source_file_name)) \n",
    "        return destination_blob_name\n",
    "        \n",
    "    \n",
    "    df_iris=read_data()\n",
    "    df_iris=feature_enginering(df_iris,quant,column_bin)\n",
    "    X_train, X_test, y_train, y_test=split_dataframe(df_iris,test_size,target)\n",
    "    X_train_path=push_gcp(X_train,bucket_name,source_file_name,destination_blob_name)\n",
    "    X_test_path=push_gcp(X_test,bucket_name,source_file_name,destination_blob_name)\n",
    "    y_train_path=push_gcp(y_train,bucket_name,source_file_name,destination_blob_name)\n",
    "    y_test_path=push_gcp(y_test,bucket_name,source_file_name,destination_blob_name)\n",
    "    \n",
    "    \n",
    "    output={'X_train_path':X_train_path\n",
    "          ,'X_test_path':X_test_path\n",
    "          ,'y_train_path',y_train_path\n",
    "          ,'y_test_path':y_test_path\n",
    "           ,'bucket':bucket}\n",
    "    read_output = namedtuple('ReadOutput', ['output'])\n",
    "    return read_output(output)\n",
    " \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    import itertools\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import logging\n",
    "    import scikitplot as skplt\n",
    "\n",
    "    from sklearn import datasets\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "    from matplotlib.backends.backend_pdf import PdfPages\n",
    "    from google.cloud import storage\n",
    "\n",
    "    \n",
    "    \n",
    "    def read_from_bucket(bucket_name:str,source_file_name:str)-> pd.DataFrame: \n",
    "        client = storage.Client()\n",
    "        bucket = client.get_bucket(bucket_name)\n",
    "        blob = bucket.get_blob(source_file_name)\n",
    "        content = blob.download_as_string()  \n",
    "        df = pd.read_csv(BytesIO(content),index_col=[0])\n",
    "        return df\n",
    "    \n",
    "    def fit_matrix(df:pd.DataFrame): \n",
    "        matrix = CountVectorizer(max_features=5000)\n",
    "        matrix.fit_transform(df.tolist())\n",
    "        return matrix\n",
    "\n",
    "    def generate_matrix_transfrom(matrix,df:pd.DataFrame): \n",
    "        array_data = matrix.transform(df.tolist()).toarray()\n",
    "        return array_data\n",
    "\n",
    "    def fit_model(model:str,X_train:pd.DataFrame,y_train:pd.DataFrame):\n",
    "        if model=='RandomForestClassifier': \n",
    "            clf = RandomForestClassifier(n_estimators=50,n_jobs=-1, random_state=0,criterion='gini')\n",
    "        clf.fit(X_train, y_train)\n",
    "        return clf \n",
    "\n",
    "    def generate_metrics(clf,X_test:pd.DataFrame,y_test:pd.DataFrame):\n",
    "        y_pred = clf.predict(X_test)\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        print(cm)\n",
    "        cr = classification_report(y_test, y_pred)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        return accuracy\n",
    "\n",
    "    def generate_report(y_true:pd.DataFrame,y_pred:pd.DataFrame,y_probas:pd.DataFrame,filename:str): \n",
    "        with PdfPages(filename) as export_pdf:\n",
    "            skplt.metrics.plot_roc_curve(y_true, y_probas)\n",
    "            export_pdf.savefig()\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "            cmap=plt.cm.Blues\n",
    "            classes=[1,2,3]\n",
    "            title='Confusion matrix'\n",
    "            cm= confusion_matrix(y_true, y_pred)\n",
    "            plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "            plt.title(title)\n",
    "            plt.colorbar()\n",
    "            tick_marks = np.arange(len(classes))\n",
    "            plt.xticks(tick_marks, classes, rotation=45)\n",
    "            plt.yticks(tick_marks, classes)\n",
    "\n",
    "            thresh = cm.max() / 2.\n",
    "            for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "                plt.text(j, i, cm[i, j],\n",
    "                         horizontalalignment=\"center\",\n",
    "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "            plt.tight_layout()\n",
    "            plt.ylabel('True label')\n",
    "            plt.xlabel('Predicted label')\n",
    "            export_pdf.savefig()\n",
    "            plt.close()\n",
    "            logging.info('Report is generated and saved as: {}'.format(filename))\n",
    "            return filename\n",
    "\n",
    "    def push_gcp(df:pd.DataFrame,bucket_name:str,source_file_name:str,destination_blob_name:str)-> str:\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.get_bucket(bucket_name)\n",
    "        blob = bucket.blob(destination_blob_name)\n",
    "        blob.upload_from_filename(source_file_name)\n",
    "        logging.info('{} is pushed to gcp'.format(source_file_name)) \n",
    "\n",
    "\n",
    "    test_size=0.25\n",
    "    quant=[0.2, 0.4, 0.6, 0.8, 1]\n",
    "    column='sepal length (cm)'\n",
    "    target='target'\n",
    "\n",
    "\n",
    "    clf=fit_model('RandomForestClassifier',X_train,y_train)\n",
    "    generate_metrics(clf,X_test,y_test)\n",
    "\n",
    "    y_true=y_test\n",
    "    y_pred=clf.predict(X_test)\n",
    "    y_probas=clf.predict_proba(X_test)\n",
    "    filename='metrics_report.pdf'\n",
    "    filename_generated_report=generated_report(y_true,y_pred,y_probas,filename)\n",
    "    #push_gcp(filename_generated_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_op = comp.func_to_container_op(read_data, base_image='tensorflow/tensorflow:1.11.0-py3')\n",
    "train_op = comp.func_to_container_op(train_model, base_image='tensorflow/tensorflow:1.11.0-py3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "      <th>sepal length (cm)&gt;5.0</th>\n",
       "      <th>sepal length (cm)&gt;5.6</th>\n",
       "      <th>sepal length (cm)&gt;6.1</th>\n",
       "      <th>sepal length (cm)&gt;6.52</th>\n",
       "      <th>sepal length (cm)&gt;7.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "147                6.5               3.0                5.2               2.0   \n",
       "148                6.2               3.4                5.4               2.3   \n",
       "149                5.9               3.0                5.1               1.8   \n",
       "\n",
       "     target  sepal length (cm)>5.0  sepal length (cm)>5.6  \\\n",
       "147     2.0                      1                      1   \n",
       "148     2.0                      1                      1   \n",
       "149     2.0                      1                      1   \n",
       "\n",
       "     sepal length (cm)>6.1  sepal length (cm)>6.52  sepal length (cm)>7.9  \n",
       "147                      1                       0                      0  \n",
       "148                      1                       0                      0  \n",
       "149                      0                       0                      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iris.tail(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp.dsl as dsl\n",
    "@dsl.pipeline(\n",
    "   name='ML pipeline',\n",
    "   description='A toy pipeline for the iris dataset'\n",
    ")\n",
    "def ml_pipeline(\n",
    "   a='a',\n",
    "   b='7',\n",
    "   c='17',\n",
    "):\n",
    "    #Passing pipeline parameter and a constant value as operation arguments\n",
    "    add_task = read_op() #Returns a dsl.ContainerOp class instance. \n",
    "    \n",
    "    #Passing a task output reference as operation arguments\n",
    "    #For an operation with a single return value, the output reference can be accessed using `task.output` or `task.outputs['output_name']` syntax\n",
    "    train_task = train_op(add_task.outputs['output'])\n",
    "\n",
    "    #For an operation with a multiple return values, the output references can be accessed using `task.outputs['output_name']` syntax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify pipeline argument values\n",
    "arguments = {'a': '7', 'b': '8'}\n",
    "\n",
    "#Submit a pipeline run\n",
    "kfp.Client().create_run_from_pipeline_func(ml_pipeline, arguments=arguments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
